---
title: "Text Mining Trump Speeches with R"
author: "Adela Sobotkova, adela@fedarch.org"
date: "24 July 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

adapted from P J Murphy @ https://rpubs.com/pjmurphy/265713

To begin, please install the needed packages for R. 
```{r eval = FALSE}
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", 
            "cluster", "igraph", "fpc")   
install.packages(Needed, dependencies=TRUE)  
```


If you get the following message:   
```{r eval=FALSE}
Update all/some/none? [a/s/n]:   
```


enter "a" and press return   

##########################################################################################
#                                  Loading Texts                                         #
##########################################################################################      

Start by saving your text files in a folder titled:    "texts"
This will be the "corpus" (body) of texts you are mining.

Next, choose the type of computer you have...



*On a Mac*, save the folder to your *desktop* and use the following code chunk:


```{r}
cname <- file.path("~", "Desktop", "texts")   
cname   
dir(cname)   # Use this to check to see that your texts have loaded.   
```


  
*On a PC*, save the folder to your *C: drive* and use the following code chunk:  

```{r}
cname <- file.path("./data", "texts")   
cname   
dir(cname)   
```

##########################################################################################
#                                Start Your Analyses                                     #
##########################################################################################
Load the R package for text mining and then load your texts into R.
```{r}
library(tm)
docs <- VCorpus(DirSource(cname))   
```

Look at one of the texts; what problems do you see ?
```{r}
writeLines(as.character(docs[1]))
```

## Pre-process the texts

Handle problematic diacritics and special symbols
```{r}
for (j in seq(docs)) {
  docs[[j]] <- gsub( "â\200\231","", docs[[j]]) # remove apostrophe
  docs[[j]] <- gsub( "â\200","-", docs[[j]])  # reinsert hyphen
    docs[[j]] <- gsub("@"," ", docs[[j]])
  docs[[j]] <- gsub("\\|", " ", docs[[j]])
  docs[[j]] <- gsub("\u2028", " ", docs[[j]]) 
  docs[[j]] <- gsub("/"," ", docs[[j]])
}
```

Clean up text from numbers, capitals, whitespaces and unwanted words 

```{r}
docs <- tm_map(docs,removePunctuation)   
docs <- tm_map(docs, removeNumbers)   
docs <- tm_map(docs, tolower)   
docs <- tm_map(docs, removeWords, stopwords("english"))   
docs <- tm_map(docs, removeWords, c("syllogism", "tautology"))   
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, PlainTextDocument)
```

# *This is the end of the preprocessing stage.*   
```{r}
writeLines(as.character(docs[1]))
```

## Stage the Data    
```{r}
dtm <- DocumentTermMatrix(docs)   
tdm <- TermDocumentMatrix(docs)   
```


## Explore your data

```{r}
freq <- colSums(as.matrix(dtm))   
length(freq)   
ord <- order(freq)   
m <- as.matrix(dtm)   
dim(m)
```


### Save the result to a file

```{r}
write.csv(m, file="DocumentTermMatrix.csv")   
```
   
   

# Let's focus on the interesting stuff...;)

Start by removing sparse terms:   

```{r}
dtms <- removeSparseTerms(dtm, 0.1) # This makes a matrix that is 10% empty space, maximum.  
```


## Calculate Word Frequency   
```{r}
head(table(freq), 20) 
```
The above output is two rows of numbers. The top number is the frequency with which
words appear and the bottom number reflects how many words appear that frequently.

```{r}
tail(table(freq), 20) 
```
Considering only the 20 greatest frequencies


View a table of the terms after removing sparse terms, as above.
```{r}
freq <- colSums(as.matrix(dtms))   
freq 
```
The above matrix was created using a data transformation we made earlier. 

**An alternate view of term frequency:**   
This will identify all terms that appear frequently (in this case, 50 or more times).  

```{r}
findFreqTerms(dtm, lowfreq=50)   # Change "50" to whatever is most appropriate for your data.
```

## Plot Word Frequencies
**Plot words that appear at least 50 times.**   
```{r}
library(ggplot2)   
wf <- data.frame(word=names(freq), freq=freq)   
p <- ggplot(subset(wf, freq>50), aes(word, freq))    
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))   
p   
```

# Relationships Between Terms
## Term Correlations
See the description above for more guidance with correlations.
If words always appear together, then correlation=1.0.    

```{r}
findAssocs(dtm, c("country" , "american"), corlimit=0.85) # specifying a correlation limit of 0.85
findAssocs(dtms, "think", corlimit=0.70) # specifying a correlation limit of 0.95   
```
 
Change "country" & "american", or "think" to terms that actually appear in your texts.
Also adjust the `corlimit= ` to any value you feel is necessary.

## Word Clouds!   
First load the package that makes word clouds in R.    

```{r}
library(wordcloud)  
```

Then play with wordclouds:)
```{r}
dtms <- removeSparseTerms(dtm, 0.15) # Prepare the data (max 15% empty space)   
freq <- colSums(as.matrix(dtm)) # Find word frequencies   
dark2 <- brewer.pal(6, "Dark2")   
wordcloud(names(freq), freq, max.words=100, rot.per=0.2, colors=dark2) 
```
 
# Clustering by Term Similarity

## Hierarchal Clustering   

```{r}
dtms <- removeSparseTerms(dtm, 0.15) # This makes a matrix that is only 15% empty space.
library(cluster)   
d <- dist(t(dtms), method="euclidian")   # First calculate distance between words
fit <- hclust(d=d, method="complete")    # Also try: method="ward.D"   
plot.new()
plot(fit, hang=-1)
groups <- cutree(fit, k=6)   # "k=" defines the number of clusters you are using   
rect.hclust(fit, k=6, border="red") # draw dendogram with red borders around the 5 clusters   
```



## K-means clustering  
Run the install.packages("fpc") command if you have not done so already above and attach the library
```{r}
library(fpc)   
library(cluster)  
```

Once the libraries are loaded, remove sparse terms from your matrix and run the term distance analysis
```{r}
dtms <- removeSparseTerms(dtm, 0.15) # Prepare the data (max 15% empty space)   
d <- dist(t(dtms), method="euclidian")   
kfit <- kmeans(d, 2)   
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0, 
         main = "Clustering of terms in Trump speeches")  
```

# Improve, Reuse and Enjoy
Good job! You made it to the end :) 
Adapt and apply this script to a text of your choice! 
