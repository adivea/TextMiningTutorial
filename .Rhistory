cname <- file.path("~", "./data/", "texts")
cname
dir(cname)
cname <- file.path("./data/", "texts")
cname
dir(cname)
?dir()
# Get packages
library(tm)
# Lets load texts to R
docs <- VCorpus(DirSource(cname))
summary(docs)
# For details about documents in the corpus, use the inspect(docs) command.
inspect(docs[1])
# To write out the full text, use writeLines()
writeLines(as.character(docs[1]))
# To write out the full text, use writeLines()
writeLines()
# To write out the full text, use writeLines()
?writeLines()
# To write out the full text, use writeLines()
?writeLines(docs[1])
# To write out the full text, use writeLines()
writeLines(docs[1])
# To write out the full text, use writeLines()
class(docs[1])
writeLines(as.character(docs[1]))
# Remove unwanted diacritics
docs <- tm_map(docs,removePunctuation)
writeLines(as.character(docs[1])) # Check to see if it worked.
for (j in seq(docs)) {
docs[[j]] <- gsub( "â\200\231","", docs[[j]])
docs[[j]] <- gsub("@"," ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
docs[[j]] <- gsub("\u2028", " ", docs[[j]])
docs[[j]] <- gsub("/"," ", docs[[j]])
}
writeLines(as.character(docs[1])) # Check to see if it worked.
for (j in seq(docs)) {
docs[[j]] <- gsub( "â\200","-", docs[[j]])  # reinsert hyphen
docs[[j]] <- gsub( "â\200\231","", docs[[j]]) # remove apostrophe
docs[[j]] <- gsub("@"," ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
docs[[j]] <- gsub("\u2028", " ", docs[[j]])
docs[[j]] <- gsub("/"," ", docs[[j]])
}
writeLines(as.character(docs[1])) # Check to see if it worked.
docs <- tm_map(docs, removeNumbers)
writeLines(as.character(docs[1])) # Check to see if it worked.
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, PlainTextDocument)
writeLines(as.character(docs[1])) # Check to see if it worked.
DocsCopy <- docs
writeLines(as.character(docs[1])) # Check to see if it worked.
# For a list of the stopwords, see:
length(stopwords("english"))
stopwords("english")
stopwords("danish")
stopwords("french")
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, PlainTextDocument)
writeLines(as.character(docs[1])) # Check to see if it worked.
docs <- tm_map(docs, removeWords, c("syllogism", "tautology", "im"))
writeLines(as.character(docs[1])) # Check to see if it worked.
for (j in seq(docs))
{
docs[[j]] <- gsub("fake news", "fake_news", docs[[j]])
docs[[j]] <- gsub("inner city", "inner-city", docs[[j]])
docs[[j]] <- gsub("politically correct", "politically_correct", docs[[j]])
}
writeLines(as.character(docs[1]))
docs <- tm_map(docs, PlainTextDocument)
writeLines(as.character(docs[1]))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, PlainTextDocument)
dtm <- DocumentTermMatrix(docs)
dtm
tdm <- TermDocumentMatrix(docs)
tdm
inspect(dtm)
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
m <- as.matrix(dtm)
dim(m)
write.csv(m, file="DocumentTermMatrix.csv")
# Remove unwanted diacritics
docs <- tm_map(docs,removePunctuation)
writeLines(as.character(docs_stc[1])) # Check to see if it worked.
writeLines(as.character(docs[1])) # Check to see if it worked.
# Remove unwanted diacritics
?tm_maps()
# Remove unwanted diacritics
?tm_map()
?removePunctuation()
dtms <- removeSparseTerms(dtm, 0.2) # This makes a matrix that is 20% empty space, maximum.
dtms
freq <- colSums(as.matrix(dtm))
head(table(freq), 20) # The ", 20" indicates that we only want the first 20 frequencies. Feel free to change that number.
tail(table(freq))
tail(table(freq), 20)
freq <- colSums(as.matrix(dtms))
freq
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
head(freq, 14)
findFreqTerms(dtm, lowfreq=50)   # Change "50" to whatever is most appropriate for your text data.
findFreqTerms(dtm, lowfreq=100)   # Change "50" to whatever is most appropriate for your text data.
findFreqTerms(dtm, lowfreq=200)   # Change "50" to whatever is most appropriate for your text data.
findFreqTerms(dtm, lowfreq=300)   # Change "50" to whatever is most appropriate for your text data.
findFreqTerms(dtm, lowfreq=400)   # Change "50" to whatever is most appropriate for your text data.
findFreqTerms(dtm, lowfreq=100)   # Change "50" to whatever is most appropriate for your text data.
wf <- data.frame(word=names(freq), freq=freq)
head(wf)
library(ggplot2)
p <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
p
########## Plot Word Clouds!
library(RColorBrewer)
set.seed(142)
wordcloud(names(freq), freq, min.freq=25)
library(wordcloud)
set.seed(142)
wordcloud(names(freq), freq, min.freq=25)
wordcloud(names(freq), freq, min.freq=25)
wordcloud(names(freq), freq, min.freq=25)
wordcloud(names(freq), freq, min.freq=100)
library(ggwordcloud)
########## Plot Word Clouds!
install.packages(ggwordcloud)
set.seed(142) # should retain the same layout
wordcloud(names(freq), freq, min.freq=100)
set.seed(142) # should retain the same layout
wordcloud(names(freq), freq, min.freq=100)
set.seed(142) # should retain the same layout
wordcloud(names(freq), freq, min.freq=100)
wordcloud(names(freq), freq, min.freq=100)
#set.seed(142) # should retain the same layout
wordcloud(names(freq), freq, min.freq=100)
#set.seed(142) # should retain the same layout
wordcloud(names(freq), freq, min.freq=100)
#set.seed(142) # should retain the same layout
wordcloud(names(freq), freq, min.freq=100)
#set.seed(142) # should retain the same layout
wordcloud(names(freq), freq, min.freq=100)
set.seed(142)
wordcloud(names(freq), freq, min.freq=20, scale=c(5, .1), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=50, scale=c(5, .1), colors=brewer.pal(6, "Dark2"))
set.seed(142)
dark2 <- brewer.pal(6, "Dark2")
wordcloud(names(freq), freq, max.words=100, rot.per=0.2, colors=dark2)
?wordcloud()
wordcloud(names(freq), freq, max.words=100, rot.per=0.5, colors=dark2)
source('~/.active-rstudio-document', encoding = 'UTF-8')
##########################################################################################
#                                Start Your Analyses                                     #
##########################################################################################
# **Load the R package for text mining and then load your texts into R.**
library(tm)
cname
inspect(docs)
docs <- tm_map(docs,removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, c("syllogism", "tautology"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, PlainTextDocument)
# *This is the end of the preprocessing stage.*
writeLines(as.character(docs)[1])
(docs)
tdm <- TermDocumentMatrix(docs)
### Stage the Data
dtm <- DocumentTermMatrix(docs)
tdm <- TermDocumentMatrix(docs)
### Explore your data
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
m <- as.matrix(dtm)
write.csv(m, file="DocumentTermMatrix.csv")
### FOCUS - on just the interesting stuff...
#  Start by removing sparse terms:
dtms <- removeSparseTerms(dtm, 0.1) # This makes a matrix that is 10% empty space, maximum.
### Word Frequency
head(table(freq), 20)
# The above output is two rows of numbers. The top number is the frequency with which
# words appear and the bottom number reflects how many words appear that frequently.
#
tail(table(freq), 20)
# Considering only the 20 greatest frequencies
#
# **View a table of the terms after removing sparse terms, as above.
freq <- colSums(as.matrix(dtms))
freq
# The above matrix was created using a data transformation we made earlier.
# **An alternate view of term frequency:**
# This will identify all terms that appear frequently (in this case, 50 or more times).
findFreqTerms(dtm, lowfreq=50)   # Change "50" to whatever is most appropriate for your data.
library(ggplot2)
wf <- data.frame(word=names(freq), freq=freq)
p <- ggplot(subset(wf, freq>50), aes(word, freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
#
## Relationships Between Terms
### Term Correlations
# See the description above for more guidance with correlations.
# If words always appear together, then correlation=1.0.
findAssocs(dtm, c("country" , "american"), corlimit=0.85) # specifying a correlation limit of 0.85
findAssocs(dtms, "think", corlimit=0.70) # specifying a correlation limit of 0.95
### Hierarchal Clustering
dtms <- removeSparseTerms(dtm, 0.15) # This makes a matrix that is only 15% empty space.
library(cluster)
d <- dist(t(dtms), method="euclidian")   # First calculate distance between words
fit <- hclust(d=d, method="complete")    # Also try: method="ward.D"
plot.new()
plot(fit, hang=-1)
groups <- cutree(fit, k=6)   # "k=" defines the number of clusters you are using
rect.hclust(fit, k=6, border="red") # draw dendogram with red borders around the 5 clusters
### K-means clustering
library(fpc)
### K-means clustering
install.packages("fpc")
library(fpc)
dtms <- removeSparseTerms(dtm, 0.15) # Prepare the data (max 15% empty space)
d <- dist(t(dtms), method="euclidian")
kfit <- kmeans(d, 2)
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
knitr::opts_chunk$set(echo = TRUE)
writeLines(as.character(docs)[1])
writeLines(as.character(docs[1]))
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0,
mean = "Clustering of terms in Trump speeches")
GREL = "value.match(/(^[0-9]{1,2}\w\w)\s(.*)/)[0]"
Excel_Formula = "INDEX(salary!$C$1:$C$4,MATCH(1,('player DB'!$A2=salary!$A$1:$A$4)*('player DB'!$B2=salary!$B$1:$B$4),0))"
df = rbind(GREL, Excel_Formula)
plot(df)
GREL = "value.match(/(^[0-9]{1,2}\w\w)\s(.*)/)[0]"
Excel_Formula = "INDEX(salary!$C$1:$C$4,MATCH(1,('player DB'!$A2=salary!$A$1:$A$4)*('player DB'!$B2=salary!$B$1:$B$4),0))"
df = rbind(GREL, Excel_Formula)
plot(df)
GREL = quotes("value.match(/(^[0-9]{1,2}\w\w)\s(.*)/)[0]")
Excel_Formula = "INDEX(salary!$C$1:$C$4,MATCH(1,('player DB'!$A2=salary!$A$1:$A$4)*('player DB'!$B2=salary!$B$1:$B$4),0))"
df = rbind(GREL, Excel_Formula)
plot(df)
par(mar = c(0, 1, 0, 1))
pie(
c(288, 72),
c('Tedious', 'Confusing'),
col = c('#0736A4', '#CC3300'),
init.angle = 54, border = T, main='\n\nData Cleaning in the BOR Era \n "Before OpenRefine"'
)
par(mar = c(0, 1, 0, 1))
pie(
c(288, 72),
c('Tedious', 'Confusing'),
col = c('#0736A4', '#CC3300'),
init.angle = 54, border = T, main='\n\nData Cleaning in the BOR Era \n "Before OpenRefine"'
)
par(mar = c(0, 1, 0, 1))
pie(
c(288, 72),
c('Tedious', 'Confusing'),
col = c('#0736A4', '#CC3300'),
init.angle = 54, border = T, main='\n\nData Cleaning in the BOR Era \n "Before OpenRefine"'
)
?gsub()
